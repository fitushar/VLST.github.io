<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>CVIT-VLST</title>  
  <link rel="icon" type="image/x-icon" href="static/images/cvit.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Virtual Lung Screening Trial (VLST): An In Silico Replica of the National Lung Screening Trial for Lung Cancer Detection</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://www.linkedin.com/in/fitushar/" target="_blank">Fakrul Islam Tushar</a>,</span>                
  <span class="author-block">Liesbeth Vancoillie,</span>
  <span class="author-block">Cindy McCabe,</span>
  <span class="author-block">Amareswararao Kavuri,</span>
  <span class="author-block">Lavsen Dahal</a>,</span>  
  <span class="author-block">Brian Harrawood,</span>
  <span class="author-block">Milo Fryling,</span>
  <span class="author-block">Mojtaba Zarei,</span>
  <span class="author-block">Saman Sotoudeh-Paima</a>,</span>
  <span class="author-block">Fong Chi Ho,</span>
  <span class="author-block">Dhrubajyoti Ghosh,</span>
  <span class="author-block">Michael R. Harowicz,</span>
  <span class="author-block">Tina D. Tailor,</span>
  <span class="author-block">Sheng Luo,</span>
  <span class="author-block">
    <a href="https://radiology.duke.edu/profile/william-paul-segars" target="_blank">W. Paul Segars</a>,
  </span>
  <span class="author-block">
    <a href="https://radiology.duke.edu/profile/ehsan-abadi" target="_blank">Ehsan Abadi</a>,
  </span>
  <span class="author-block">
    <a href="https://radiology.duke.edu/profile/kyle-jon-lafata" target="_blank">Kyle J. Lafata</a>,
  </span>
  <span class="author-block">
    <a href="https://radiology.duke.edu/profile/joseph-yuan-chieh-lo" target="_blank">Joseph Y. Lo/a><sup>+</sup>,
  </span>
  <span class="author-block">
    <a href="https://radiology.duke.edu/profile/ehsan-samei" target="_blank">Ehsan Samei</a><sup>+</sup>
  </span>
</div>
                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Center for Virtual Imaging Trials, Carl E. Ravin Advanced Imaging Laboratories, Department of Radiology,<be>Duke University School of Medicine, Durham, NC, 27708, USA</span>
                    <span class="eql-cntrb"><small><br><sup>+</sup>Indicates Co-Senior Authors</span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/abs/2404.11221" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- SPIE Phy. MedI 2024 abtarct link -->
                    <span class="link-block">
                      <a href="https://doi.org/10.1117/12.3006915" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-globe"></i>
                      </span>
                      <span>SPIE Phy. MedI(2024)</span>
                    </a>
                  </span>

                    <!-- VITM 2024 -->
                    <span class="link-block">
                      <a href="https://www.researchgate.net/publication/380373653_Beyond_Detection_Bridging_the_Gap_Between_Nodule_Detection_and_Lung_Cancer_Diagnosis_in_Virtual_Imaging_Trials_VITs" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-award"></i>
                      </span>
                      <span>VITM(2024)</span>
                    </a>
                  </span>

                    <!-- RSNA abtarct link -->
                    <span class="link-block">
                      <a href="https://github.com/fitushar/VLST.github.io/blob/master/static/pdfs/Fitushar_et_al_RSAN2024_VLST.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-globe"></i>
                      </span>
                      <span>RSNA 2024</span>
                    </a>
                  </span>                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://cvit.duke.edu/resources/" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Resources</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2404.11221" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <source src="static/videos/VLST_workflow_video.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Virtual Lung Screening Trial (VLST) workflow, showcasing AI-driven in silico trials in lung cancer screening—part of CVIT’s Monthly Forum. Watch the full forum <a href="https://cvit.duke.edu/forum/june-21-2024-unlocking-the-power-of-ai-in-silico-trials-in-chest-radiology/">here</a>.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            <strong>Importance:</strong> Clinical imaging trials are crucial for evaluation of medical innovations, but the process is inefficient, expensive, and ethically-constrained. Virtual imaging trial (VIT) approach addresses these
limitations by emulating the components of a clinical trial. An in silico rendition of the National Lung
Screening Trial (NCLS) via Virtual Lung Screening Trial (VLST) demonstrates the promise of VITs to
expedite clinical trials, reduce risks to subjects, and facilitate the optimal use of imaging technologies in
clinical settings.
<strong>Objectives:</strong> To demonstrate that a virtual imaging trial platform can accurately emulate a major clinical
trial, specifically the National Lung Screening Trial (NLST) that compared computed tomography (CT)
and chest radiography (CXR) imaging for lung cancer screening.
<strong>Design, Setting, and Participants:</strong> A virtual patient population of 294 subjects was created from human
models (XCAT) emulating the NLST, with two types of simulated cancerous lung nodules. Each virtual
patient in the cohort was assessed using simulated CT and CXR systems to generate images reflecting the
NLST imaging technologies. Deep learning models trained for lesion detection, AI CT-Reader, and AI
CXR-Reader served as virtual readers.
<strong>Main Outcomes and Measures:</strong> The primary outcome was the difference in the Receiver Operating
Characteristic Area Under the Curve (AUC) for CT and CXR modalities.
<strong>Results:</strong> The study analyzed paired CT and CXR simulated images from 294 virtual patients. The AI CTReader outperformed the AI CXR-Reader across all levels of analysis. At the patient level, CT
demonstrated superior diagnostic performance with an AUC of 0.92 (95% CI: 0.90-0.95), compared to
CXR’s AUC of 0.72 (0.67-0.77). Subgroup analyses of lesion types revealed CT had significantly better detection of homogeneous lesions (AUC 0.97, 95% CI: 0.95-0.98) compared to heterogeneous lesions
(0.89; 0.86-0.93). Furthermore, when the specificity of the AI CT-Reader was adjusted to match the
NLST sensitivity of 94% for CT, the VLST results closely mirrored the NLST findings, further
highlighting the alignment between the two studies.
<strong>Conclusion and Relevance:</strong> The VIT results closely replicated those of the earlier NLST, underscoring
its potential to replicate real clinical imaging trials. Integration of virtual trials may aid in the evaluation
and improvement of imaging-based diagnosis.          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/VLST_Figure2.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
          Virtual Patient’s inclusion and exclusion criteria and progress through the study. Within the study, 294 virtual patients were assessed, with 174 of them having a total of 512 lesions, varying from homogeneous to heterogeneous in nature. All 294 virtual patients underwent both virtual CT and CXR scans. For the CT cohort, *294 virtual images were processed through two distinct scanners, the Duke Legacy-12 and the Duke Legacy-20, with each scanner producing three unique imaging configurations per patient (294 x 3=882). **From these six configurations, one CT image per patient, total 294 was randomly selected for evaluation. As for the CXR cohort, all 294 virtual patients were successfully imaged using the Duke Legacy-CXR scanner. A indicate the area under the receiver operating characteristic curve.        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/VLST_Figure3.png" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered">
         Example of human model and simulated images from the Virtual Lungs Screening Trial. Selected slice of (A) computation human model with a homogenous lesion (B) simulated CT scan image from Duke Legacy W20 scanner C) simulated CXR image using legacy post-processing.       </h2>
     </div>
     <div class="item">
      <!-- Your image here -->
      <img src="static/images/VLST_Figure4.png" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered">
        Performance of the AI CT-Reader and AI CXR-Reader in predicting lung cancer across patient and lesion types. (a) Receiver operating characteristic (ROC) curves comparing the performance of the AI CT-Reader and AI CXR-Reader for patient-level predictions, homogeneous lesions, and heterogeneous lesions. Blue and brown lines represent AI CT-Reader and AI CXR-Reader performance, respectively. Homogeneous lesions are represented by a solid line, and heterogeneous lesions by a dashed line. The diagonal dashed line represents the ROC curve for a random classifier with an AUROC of 0.50. NLST CT and NLST CXR results are marked with blue and brown dot (error bar represents 95% CI of sensitivity and specificity), respectively. (b) Forest plot showing the difference in AUROC between AI CT-Reader and AI CXR-Reader for patient-level predictions. The blue and brown markers represent the AUROC differences for CT and CXR, with error bars indicating the 95% confidence intervals. The p-value for statistical significance is shown (p < 0.001).
AI = artificial intelligence. ROC = receiver operating characteristic. AUROC = area under the receiver operating characteristic curve. NLST = National Lung Screening Trial.
      </h2>
    </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->

<!-- Youtube video -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <h2 class="title is-3">Project Presentation-VLST</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video">
            <!-- Youtube embed code here -->
            <iframe src="https://www.youtube.com/embed/c6WyWRhaSHg?si=hYuv1Mu9_tGah6uS" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End youtube video -->

<!-- Youtube video -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <!-- Paper video. -->
      <h2 class="title is-3">VLST-NotebookLM Podcast</h2>
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          
          <div class="publication-video">
            <!-- Youtube embed code here -->
            <iframe src="https://www.youtube.com/embed/z9VpTohEzVM?si=NRERqnsq_eo8uN44" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End youtube video -->










<!-- Paper poster -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Pre-print</h2>

      <iframe  src="static/pdfs/2404.11221v3.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section>
<!--End paper poster -->

  <!-- Paper poster -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">SPIE Phy. MedI(2024) Poster</h2>

      <iframe  src="static/pdfs/SPIE24_Poster_VLST_PDF.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section>
<!--End paper poster -->

<!-- Paper poster -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Virtual Imaging Trials in Medicine – International Summit (VITM24) Poster</h2>
      
      <!-- Subheading for Award -->
      <h3 class="subtitle">
        <span class="icon">
          <i class="fas fa-award"></i> <!-- Font Awesome Award Icon -->
        </span>
        Won the Best Poster Award
      </h3>

      <!-- Poster PDF Embed -->
      <iframe src="static/pdfs/Beyond_detection_VITs_vitm24_poster.pdf" width="100%" height="550"></iframe>
    </div>
  </div>
</section>
<!-- End paper poster -->

  <!-- Paper poster -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">RSNA Annual Meeting (2024) Abstarct</h2>

      <iframe  src="static/pdfs/Fitushar_et_al_RSAN2024_VLST.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section>
<!--End paper poster -->

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{tushar2024virtuallungscreeningtrial,
      title={Virtual Lung Screening Trial (VLST): An In Silico Replica of the National Lung Screening Trial for Lung Cancer Detection}, 
      author={Fakrul Islam Tushar and Liesbeth Vancoillie and Cindy McCabe and Amareswararao Kavuri and Lavsen Dahal and Brian Harrawood and Milo Fryling and Mojtaba Zarei and Saman Sotoudeh-Paima and Fong Chi Ho and Dhrubajyoti Ghosh and Michael R. Harowicz and Tina D. Tailor and Sheng Luo and W. Paul Segars and Ehsan Abadi and Kyle J. Lafata and Joseph Y. Lo and Ehsan Samei},
      year={2024},
      eprint={2404.11221},
      archivePrefix={arXiv},
      primaryClass={eess.IV},
      url={https://arxiv.org/abs/2404.11221}, 
}</pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
